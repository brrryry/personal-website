---
title: "osu analyzer: the pilot episode"
date: "2025-01-03"
description: "the (theoretical) beginning of a deep dive into osu tool development"
tags: ["project", "coding", "personal", "ml"]
---

happy new year! welcome to 2025 :))

today, I am here to talk about a project that i thought of off the top of my head.
this is the first blog in which i will be documenting the project as i go as opposed to writing about it after the fact.
let's see how this goes!

<br />

## osu: an introduction
if you know what osu is, you can skip this section. \
\
so, what...is is osu? \
\
well...osu is a rhythm game. you click circles to the beat of a song. \
it's a pretty simple concept, but it can get hard pretty quickly. 

// insert image of osu gameplay here \
\
osu has a pretty big community, and there are a lot of custom maps that people make.
these maps can range from easy to hard, and they can be made for any song. \
\
(shameless self-plug but) you can find my profile <a href="https://osu.ppy.sh/users/11781698" target="_blank">here</a>!


<br />

## so what's the idea?
well, previously, i wanted to make a tool that could generate osu beatmaps. the current progress of this project can be found 
<a href="https://github.com/brrryry/osu-beatmap-generator" target="_blank">here</a>. however, i realized that this project was a bit 
too ambitious for me - especially since i'm not too experienced with deep learning yet (i'm taking a class on it this semester though!
expect a blog on it in the future!!!). \
\
right now, i want to make a tool that can analyze an osu beatmap and analyze the type of map it is.
this is a pretty vague idea, but i think it's a good starting point. \
\
off the top of my head, i can think of a few categories that a beatmap can fall into:

<BlogList>
<li>jump maps - maps that frequently require you to "jump" between circles</li>
<li>stream maps - maps that frequently require you to "stream"</li>
<li>tech maps - maps that purposely don't follow a pattern</li>
<li>speed maps - maps that are just...fast.</li>
<li>practice maps - maps that heavily focus on one aspect of gameplay</li>
</BlogList>

of course, these categories are not mutually exclusive. a map can be a mix of all of these categories. 
it may not also be possible to even categorize maps this way. there's only one way to find out: by trying!

<br />

## fetching maps & extracting features 
the first step to this project is to gather a dataset of osu beatmaps. 
honestly, fetching the maps is probably the easiest part. i will use <a href="https://beatconnect.io">beatconnect</a> to download a bunch of beatmaps.
while the api is not publicly available at the moment, i can use their direct download links to download maps. \
\
the question would be: how do i analyze these maps? 
for now, i will only get the map files (not the audio files). from there, i will generate a bunch of features that i think are important. 

<BlogCode language="python" title="fetch_maps.py">
{`
###########################
# This script will collect osu! maps from an osu! API (beatconnect.io) and extract them.
# The directories to each section (audio, maps) can be edited in the config file. 
###########################


# Python library imports
import requests
import zipfile
import io
import os
import random
import datetime
from concurrent.futures import ThreadPoolExecutor

# Osu API
import osu
from osu import Client

# import config
import config

# file-specific configurations!!
NUM_MAPS = 1000 #number of maps to fetch (including what is already there)
BATCH_SIZE = 25 #number of maps to fetch at a time using ThreadPoolExecutor


# Set up the osu! API client
client = Client.from_credentials(config.osu_api_client_id, config.osu_api_client_secret, config.osu_api_redirect_uri)


# Function to print with timestamp
def tsprint(s):
    print("[" + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "] " + s)


# Fetcher function to fetch a map
def fetch_map(map_id):
    tsprint(f'Fetching map {map_id}...')

    # Try to fetch the map
    try:
        # Fetch the map
        response = requests.get(config.api_link + map_id)
        response.raise_for_status()
        tsprint(f'Successfully fetched map {map_id}!')
    except:
        tsprint(f'Failed to fetch map {map_id} - got status code {response.status_code}')
        return
    
    # Extract the map in memory
    try:
        zip_file = zipfile.ZipFile(io.BytesIO(response.content))
    except:
        tsprint(f'Failed to extract map {map_id} - not a valid zip file')
        return
    
    count = 0 #count the number of osu files extracted from the zip

    # Loop through the files in the zip and extract the .osu files
    for file in zip_file.namelist():
        if file.endswith('.osu'):
            zip_file.extract(file, config.map_folder)


            # Confirm that the file is the right type of map (not mania or taiko)
            with open(config.map_folder + file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
                # find the line that specifies the mode
                mode_line = [line for line in lines if line.startswith("Mode:")]
                if len(mode_line) == 0:
                    tsprint(f'Failed to find mode line in {file}')
                    os.remove(config.map_folder + file)
                    return
                
                # check if the mode is standard
                mode = int(mode_line[0].split(":")[1].strip())
                if mode != 0:
                    tsprint(f'Map {file} is not a standard map')
                    os.remove(config.map_folder + file)
                    return
            
            # Rename the file to the map_id + count + .osu
            try:
                os.rename(config.map_folder + file, config.map_folder + map_id + "_" + str(count) + ".osu")
                count += 1
            except:
                tsprint(f'Failed to rename file {file} to {map_id}_{count}.osu')
                os.remove(config.map_folder + file) #remove the file if it cannot be renamed
                return

    tsprint(f'Successfully extracted {count} osu files from map {map_id}!')
    return



# Function to fetch maps!
def fetch_maps(num_maps = 100):
    while True:
        count = 0
        file_count = os.listdir(config.map_folder)
        if len(file_count) >= num_maps:
            break

        # fetch maps in batches
        filter = osu.util.BeatmapsetSearchFilter()
        filter.set_mode(osu.GameModeInt.STANDARD)
        filter.set_status(osu.BeatmapsetSearchStatus.RANKED)  
        filter.set_sort(osu.BeatmapsetSearchSort.PLAYS)

        # get the top 1000 maps
        beatmapsearchresult = client.search_beatmapsets(filter, BATCH_SIZE)

        tsprint(f'Fetched {beatmapsearchresult.total} mapsets from the osu! API')


        map_ids = [beatmapset.id for beatmapset in beatmapsearchresult.beatmapsets]
        
        if beatmapsearchresult.total > num_maps:
            map_ids = map_ids[:num_maps]
            tsprint(f'Only fetching {num_maps} mapsets!')

        # use concurrent threads to fetch maps
        with ThreadPoolExecutor() as executor:

            # call fetcher function on each map id concurrently
            futures = [executor.submit(fetch_map, str(map_id)) for map_id in map_ids]

            # wait for all threads to finish
            for future in futures:
                future.result()
    
    tsprint(f'Successfully fetched {num_maps} maps!')

# Main function...that's it
if __name__ == "__main__":

    # create maps folder if it does not exist
    if not os.path.exists(config.map_folder):
        os.makedirs(config.map_folder)

    #fetch this stuff
    fetch_maps(NUM_MAPS)
`}
</BlogCode>

this code ensures that 1000 osu beatmaps are fetched and extracted. 
these maps are also guaranteed to be the most played maps.
the code utilizes the <a href="https://github.com/Sheppsu/osu.py" target="_blank">
osu.py</a> library to fetch the beatmaps. this way, i can ensure that the maps are both
standard and ranked. \
\
okay, so now that i have the maps, let's extract some features! \
\
according to <a href="https://osu.ppy.sh/community/forums/topics/794851?n=4" target="_blank">this osu forum post that i found from 6 years ago</a>,
a user by the name of <a href="https://osu.ppy.sh/users/3085123" target="_blank">Rikii</a> had a somewhat fleshed out idea of what features to extract.

<BlogImage
src="/blogs/osu-analyzer-1/rikii.png"
alt="image of rikii's post"
caption="rikii's post about map features"
source="https://osu.ppy.sh/community/forums/topics/794851?n=4"
/>

for now, i won't do tech maps (those are bit complicated). \
\
here are the features that i will extract:

<BlogList>
<li>overall stats (ar, cs, etc.)</li>
<li>jumps (number of small/medium/large jumps, jump density, etc.)</li>
<li>streams/bursts (number of streams, stream length, etc.)</li>
</BlogList>

we'll start with this. let's write the program!

<BlogCode language="python" title="extract_features.py">
{`
###########################
# This script will take osu! maps and extract features from them.
# The directories to each section can be edited in the config file. 
###########################

# Python library imports
import os
import sys
import random
import datetime
import pandas as pd
import config

# File-specific configurations
JUMP_DISTANCE_THRESHOLD = 120
JUMP_BEAT_THRESHOLD = 1 # 1 beat

BURST_DISTANCE_THRESHOLD = 60
BURST_BEAT_THRESHOLD = 1/4 # 16th notes

SPEED_BEAT_THRESHOLD = 1/2 # 8th notes

FEATURES = [
    "jump_count",
    "small_jumps_instances",
    "medium_jumps_instances",
    "large_jumps_instances",
    "jump_density",

    "burst_count",
    "burst_instances",
    "burst_density",

    "stream_count",
    "stream_instances",
    "stream_density",

    "hp_drain",
    "circle_size",
    "overall_difficulty",
    "approach_rate",
    "slider_multiplier",
    "slider_tick_rate"
]

# Function to print with timestamp
def tsprint(s):
    print("[" + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "] " + s)


def extract_features_from_folder(maps_path):
    # Get all the maps
    tsprint("Extracting features from maps...")
    maps = os.listdir(maps_path)

    # Generate pandas dataframe
    df = pd.DataFrame(columns = ["map_id"] + FEATURES)

    # Loop through the maps
    for m in maps:
        # Get the map path
        map_file = os.path.join(maps_path, m)
        features = extract_features(map_file)

        # Add the map and its features to the dataframe
        df.loc[len(df)] = [m] + [features[f] for f in FEATURES]
    
    # Save the dataframe to a csv file
    df.to_csv(config.extraction_file, index=False)

def extract_features(map_file):
    flag = False
    with open(map_file, "r", encoding="utf-8") as f:
        lines = f.readlines()

    # Get the bpms (and their switches)
    timing_points = []
    for line in lines:
        if line.startswith("[TimingPoints]"):
            flag = True
            continue
        
        if(flag):
            if line.startswith("[") or line == "\\n" or line == " \\n":
                flag = False
                break
            timing_point = {}
            timing_point["time"] = float(line.split(",")[0])
            timing_point["beat_length"] = float(line.split(",")[1])
            timing_point["meter"] = int(line.split(",")[2])

            timing_points.append(timing_point)
    
    # Get difficulty stats
    diff = {}
    for line in lines:
        if line.startswith("[Difficulty]"):
            flag = True
            continue

        if(flag):
            if line.startswith("[") or line == "\\n" or line == " \\n":
                flag = False
                break
            diff[line.split(":")[0]] = line.split(":")[1].strip()

        
    # Get the hit objects
    hit_objects = []
    for line in lines:
        if line.startswith("[HitObjects]"):
            flag = True
            continue

        if(flag):
            if line.startswith("[") or line == "\n" or line == " \n":
                flag = False
                break
            hit_object = {}
            hit_object["x"] = int(line.split(",")[0])
            hit_object["y"] = int(line.split(",")[1])
            hit_object["time"] = int(line.split(",")[2])
            hit_object["type"] = int(line.split(",")[3])
            hit_object["is_hitcircle"] = hit_object["type"] & 1
            hit_object["is_slider"] = hit_object["type"] & 2
            hit_object["is_spinner"] = hit_object["type"] & 8

            hit_objects.append(hit_object)
        
    tsprint(f"Map {map_file.split("/")[1]} has {len(timing_points)} timing points and {len(hit_objects)} hit objects. Starting feature extraction...")


    ##############################
    # Features to extract
    ##############################


    features = {}

    # Overall features
    features["hp_drain"] = float(diff["HPDrainRate"]) if "HPDrainRate" in diff else -1
    features["circle_size"] = float(diff["CircleSize"]) if "CircleSize" in diff else -1
    features["overall_difficulty"] = float(diff["OverallDifficulty"]) if "OverallDifficulty" in diff else -1
    features["approach_rate"] = float(diff["ApproachRate"]) if "ApproachRate" in diff else -1
    features["slider_multiplier"] = float(diff["SliderMultiplier"]) if "SliderMultiplier" in diff else -1
    features["slider_tick_rate"] = float(diff["SliderTickRate"]) if "SliderTickRate" in diff else -1

    # Jump features
    jump_count = 0
    small_jumps_instances = 0
    medium_jumps_instances = 0
    large_jumps_instances = 0
    jump_density = 0

    # deep copy timing points
    timing_points_copy = []
    for tp in timing_points:
        timing_points_copy.append(tp.copy())

    timing_points_copy = list(filter(lambda x: x["beat_length"] > 0, timing_points_copy))
    
    jumps = 0

    i = 0
    while i < len(hit_objects) - 1:

        while hit_objects[i]["time"] > timing_points_copy[0]["time"]:
            if len(timing_points_copy) == 1:
                break
            timing_points_copy.pop(0)

        current_beat_length = timing_points_copy[0]["beat_length"]

        distance = ((hit_objects[i]["x"] - hit_objects[i + 1]["x"]) ** 2 + (hit_objects[i]["y"] - hit_objects[i + 1]["y"]) ** 2) ** 0.5 # use euclidean distance
        time = hit_objects[i + 1]["time"] - hit_objects[i]["time"]

        # if a jump is found, keep checking for more consecutive jumps
        while distance > JUMP_DISTANCE_THRESHOLD and time < JUMP_BEAT_THRESHOLD * current_beat_length and i < len(hit_objects) - 2:
            jumps += 1
            jump_count += 1
            i += 1
            
            distance = ((hit_objects[i]["x"] - hit_objects[i + 1]["x"]) ** 2 + (hit_objects[i]["y"] - hit_objects[i + 1]["y"]) ** 2) ** 0.5
            time = hit_objects[i + 1]["time"] - hit_objects[i]["time"]

        if jumps > 0:
            if jumps < 3:
                small_jumps_instances += 1
            elif jumps < 6:
                medium_jumps_instances += 1
            else:
                large_jumps_instances += 1
        
        jumps = 0
        i += 1
    
    jump_density = jump_count / len(hit_objects)

    features["jump_count"] = jump_count
    features["small_jumps_instances"] = small_jumps_instances
    features["medium_jumps_instances"] = medium_jumps_instances
    features["large_jumps_instances"] = large_jumps_instances
    features["jump_density"] = jump_density

    # Burst/Stream features
    bursts = 0
    burst_count = 0
    burst_instances = 0
    burst_density = 0

    stream_count = 0
    stream_instances = 0
    stream_density = 0
    
    timing_points_copy = []
    for tp in timing_points:
        timing_points_copy.append(tp.copy())

    timing_points_copy = list(filter(lambda x: x["beat_length"] > 0, timing_points_copy))

    i = 0
    while i < len(hit_objects) - 1:

        while hit_objects[i]["time"] > timing_points_copy[0]["time"]:
            if len(timing_points_copy) == 1:
                break
            timing_points_copy.pop(0)
        
        current_beat_length = timing_points_copy[0]["beat_length"]

        distance = ((hit_objects[i]["x"] - hit_objects[i + 1]["x"]) ** 2 + (hit_objects[i]["y"] - hit_objects[i + 1]["y"]) ** 2) ** 0.5
        time = hit_objects[i + 1]["time"] - hit_objects[i]["time"]

        while distance < BURST_DISTANCE_THRESHOLD and time < JUMP_BEAT_THRESHOLD * current_beat_length and i < len(hit_objects) - 2:
            bursts += 1
            i += 1

            distance = ((hit_objects[i]["x"] - hit_objects[i + 1]["x"]) ** 2 + (hit_objects[i]["y"] - hit_objects[i + 1]["y"]) ** 2) ** 0.5
            time = hit_objects[i + 1]["time"] - hit_objects[i]["time"]
        
        if bursts > 7:
            stream_count += bursts
            stream_instances += 1
        elif bursts >= 3:
            burst_count += bursts
            burst_instances += 1
        
        bursts = 0
        i += 1

    burst_density = burst_count / len(hit_objects)
    stream_density = stream_count / len(hit_objects)

    features["burst_count"] = burst_count
    features["burst_instances"] = burst_instances
    features["burst_density"] = burst_density
    
    features["stream_count"] = stream_count
    features["stream_instances"] = stream_instances
    features["stream_density"] = stream_density


    return features




if __name__ == "__main__":
    extract_features_from_folder(config.map_folder)
`}
</BlogCode>

this code extracts the features that we want from the map. \
\
now, here's the hard part: clustering.

<br/>

## clustering the maps (attempt 1): kmeans
excluding the tech maps, i would like to cluster the maps into the categories that i mentioned earlier. \
practice maps depend on how "outlierish" the maps are, so i don't need a cluster for those at the moment.
therefore, i just need to cluster the maps into jump, stream, and speed maps. \
\
since i don't have speed features yet, i will only cluster the maps into jump and stream maps. \
this makes things a bit easier to test out! \
\
in order to pre-process the data, i will do some stuff:

<BlogList>
<li>remove maps that are less than 4 stars in difficulty</li>
<li>normalize features</li>
</BlogList>

<BlogCode language="python" title="create_clusters.py">
{`
###########################
# This script will take extracted features and cluster them.
# The directories to each section can be edited in the config file.
###########################

# Python library imports
import os
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt 
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# take file and cluster
df = pd.read_csv("extracted_data.csv")
df = df.dropna()

# drop map_id
df = df.drop(columns=["map_id"])

# drop all rows where overall difficulty < 5
df = df[df["overall_difficulty"] > 4]

# scale the data
scaler = StandardScaler()
scaler.fit(df)
df_scaled = scaler.transform(df)
`}
</BlogCode>

now, let's try clustering the data. \
\
we will be using kmeans clustering for this. but...what is kmeans clustering? \
\
kmeans clustering is a type of unsupervised learning algorithm that groups similar data points together. 
it is a centroid-based algorithm, where the goal is to partition the data into k clusters.
the algorithm works over iterations! it repeats the following steps until the centroids do not change (converge):

<BlogList>
<li>assign each data point to the nearest centroid</li>
<li>update the centroids to the mean of the data points assigned to it</li>
</BlogList>

initially, the number of k centroids are decided by the user (in our instance, we want 1 cluster for each map type).
each centroid has a coordinate in the n-dimensional feature space (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>). 
each data point is assigned to the nearest centroid (usually by euclidean distance). 
then, the centroids are updated to the mean of the data points assigned to it. \
\
the algorithm is sensitive to the initial placement of the centroids. 
because of this, the algorithm is usually run multiple times with different initializations. \
\
anyways, math stuff aside - let's cluster the data!

<BlogCode language="python" title="create_clusters.py">
{`
kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0)
kmeans_clusters = kmeans.fit_predict(df_scaled)
`}
</BlogCode>

i wanted to use a pair plot to visualize the clusters, but i realized that doing that with 15 features is a bit too much (it would kill my computer).
therefore, i will limit the number of features being shown at a time.
for now, let's use ``overall_difficulty``, ``burst_density``, ``stream_density`` and ``jump_density``.

<BlogCode language="python" title="create_clusters.py">
{`
# only keep certain features for pair plot
df = df[['overall_difficulty', 'burst_density', 'stream_density', 'jump_density']]

# add the clusters to the dataframe
df["kmeans_cluster"] = kmeans_clusters


print("plotting")
# plot one plot containing the clusters
sns.pairplot(df, hue="kmeans_cluster")
plt.show()
`}
</BlogCode>

doing so, we get the resulting pair plot!

<BlogImage
src="/blogs/osu-analyzer-1/pairplot_kmeans2_1.png"
alt="pair plot of the clusters"
caption="pair plot of the clusters"
/>

based off the univariate distributions, it seems like the clusters are centered around the difficulty feature.
however, we do not want this to be the case. therefore, we remove the difficulty feature in pre-processing and re-cluster the data.

<BlogImage
src="/blogs/osu-analyzer-1/pairplot_kmeans2_2.png"
alt="pair plot of the clusters"
caption="pair plot of the clusters without the difficulty feature"
/>

okay...not exactly what we want. the clusters are not too distinct. why is this? \
\
let's try scaling the data and clustering it again. i also threw in a few more features in the pair plot to see if that would help us.

<BlogImage
src="/blogs/osu-analyzer-1/pairplot_kmeans2_3.png"
alt="pair plot after scaling the data"
caption="pair plot after scaling the data"
/>

it looks like the ``approach_rate`` and ``jump_density`` features are the most important in clustering the data at the moment. 
at least the jump density is doing its job! \
\
now, let's try clustering the data with one more feature: ``fast_density``.
this feature is the number of notes that are hit within a certain time frame (for now, i set it to a 185ms window). \
\
let's see how this goes!

<BlogImage
src="/blogs/osu-analyzer-1/pairplot_kmeans2_4.png"
alt="pair plot after adding the fast density feature (scaled)"
caption="pair plot after adding the fast density feature (scaled)"
/>

woah, this is better! however... \
\
something to notice is that the clusters are not distinct. this is hard to see. how about we re-evaluate? 

<br />

## clustering the maps (part 2): dbscan
instead of using kmeans, let's try using dbscan. 
density-based spatial clustering of applications with noise (dbscan) is a density-based clustering algorithm that groups together points that are closely packed together.
it is a bit more robust to noise and can find clusters of arbitrary shapes. 
this is particularly useful for our data, as most maps are not going to be specifically "jump" or "stream" maps. \
\
with kmeans, the problem was that the clusters were not distinct. dbscan should help with this. \
\
// insert dbscan explanation here \
\
instead of using the previously stated number of features (there were a lot of them), let's try using weighted features to simplify. \
\
i will only use two features in the clustering algorithm: ``jump_confidence`` and ``stream_confidence``. 
however, these features will be calculated based on our previously created features.
